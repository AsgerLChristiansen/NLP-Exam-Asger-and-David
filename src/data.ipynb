{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If youâ€™re happy with the dataset, then load it\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from itertools import islice\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "Cross validation\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349\n",
    "text summarization\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961\n",
    "More text summarization\n",
    "\n",
    "\n",
    "PREPROCESSING DECISIONS:\n",
    "\n",
    "1. Labels at every tenth percentile.\n",
    "\n",
    "2. Max input sequence length - both standard BERT and max-doc-length as separate models.\n",
    "\n",
    "3. New-lines removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "tifu_short_raw = load_dataset(\"reddit_tifu\", \"short\", split=\"train\")\n",
    "tifu_long_raw = load_dataset(\"reddit_tifu\", \"long\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decimal labels to upvote ratio\n",
    "def add_columns(example):\n",
    "    example[\"upvote_ratio\"] = round(example[\"upvote_ratio\"], 1)\n",
    "    example[\"labels\"] = int(example[\"upvote_ratio\"] * 10)\n",
    "    return example\n",
    "\n",
    "tifu_short = tifu_short_raw.map(add_columns)\n",
    "\n",
    "tifu_long = tifu_long_raw.map(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset_short for rows without a tldr\n",
    "\n",
    "tifu_no_tldr = tifu_short.filter(lambda example: example[\"tldr\"] == \"\")\n",
    "\n",
    "len(tifu_no_tldr) == len(tifu_short) - len(tifu_long) # True. Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create smaller chunks of the no_tldr dataset\n",
    "\n",
    "small_no_tldr_1 = tifu_no_tldr.select(list(range(10000)))\n",
    "small_no_tldr_2 = tifu_no_tldr.select(list(range(10000, 20000)))\n",
    "small_no_tldr_3 = tifu_no_tldr.select(list(range(20000, len(tifu_no_tldr))))\n",
    "\n",
    "print(len(small_no_tldr_1), len(small_no_tldr_2), len(small_no_tldr_3))\n",
    "# Generate tldr for 10.000 posts\n",
    "from summarizer import TransformerSummarizer, Summarizer\n",
    "bert_model = Summarizer()\n",
    "\n",
    "# Add decimal labels to upvote ratio\n",
    "def gen_tldr(example):\n",
    "    raw_text = example[\"documents\"]\n",
    "    cleaner_text = raw_text.replace(\"\\n\\n\", '.')\n",
    "    clean_text = raw_text.replace('\\n', '.')\n",
    "    bert_summary = ''.join(bert_model(clean_text, num_sentences = 2))\n",
    "    example[\"tldr\"] = bert_summary\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 1st 10.000 tldr's.\n",
    "gen_tldr_1 = small_no_tldr_1.map(gen_tldr)\n",
    "gen_tldr_1.save_to_disk(\"preprocessed/gen_tldr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the 2nd 10.000 tldr's.\n",
    "\n",
    "gen_tldr_2 = small_no_tldr_2.map(gen_tldr)\n",
    "gen_tldr_2.save_to_disk(\"preprocessed/gen_tldr_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "\n",
    "# Split into train, validation and test.\n",
    "dict1 = tifu_short.train_test_split(test_size=0.1)\n",
    "\n",
    "train_and_val = dict1[\"train\"]\n",
    "test_short = dict1[\"test\"]\n",
    "\n",
    "dict2 = train_and_val.train_test_split(test_size=0.2)\n",
    "\n",
    "train_short = dict2[\"train\"]\n",
    "val_short = dict2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"documents\"], padding = True, truncation= True, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "train_input = train_short.map(tokenization, batched = True)\n",
    "val_input = val_short.map(tokenization, batched = True)\n",
    "test_input = test_short.map(tokenization, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.save_to_disk(\"preprocessed/train_short\")\n",
    "val_input.save_to_disk(\"preprocessed/val_short\")\n",
    "test_input.save_to_disk(\"preprocessed/test_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a model!\n",
    "\n",
    "def prepare_batch(token_ids, labels):\n",
    "    token_id_tensor = torch.LongTensor(np.array(token_ids)) # Turn list of lists into np.array into torch.LongTensor\n",
    "    labels = torch.LongTensor(np.array(labels))\n",
    "    output = tuple([token_id_tensor, labels])\n",
    "    return output\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_cl_short = load_from_disk(\"../preprocessed/train_short\")\n",
    "val_cl_short = load_from_disk(\"../preprocessed/val_short\")\n",
    "test_cl_short = load_from_disk(\"../preprocessed/test_short\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_cl_short = train_cl_short.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "train_cl_short.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "small_train = train_cl_short.select(range(1000))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(small_train, batch_size=32)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=11)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "nosave = 0\n",
    "\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO:] Training classifier...\")\n",
    "\n",
    "k = 0\n",
    "for epoch in range(epochs):\n",
    "    ## for each batch\n",
    "    for batch in train_dataloader:\n",
    "        k += 1\n",
    "        print(\"This is working for the \", k, \"th time!\")\n",
    "        ## train on one batch\n",
    "            # FORWARD PASS\n",
    "        y = model(**batch)\n",
    "            # Calculate loss\n",
    "        loss = y.loss\n",
    "            # backpropagation\n",
    "        loss.backward()\n",
    "            # take step, reset\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Validate model\n",
    "    #val_X = val_idx\n",
    "    #val_y = model(val_X)\n",
    "        # Calculate validation loss\n",
    "    #val_loss = model.loss_fn(outputs=val_y, labels=val_tags)\n",
    "        # If best_val_loss exists, see if it needs updating.\n",
    "    #try:\n",
    "    #    if best_val_loss > val_loss.item():\n",
    "    #        best_val_loss = val_loss.item()\n",
    "    #            # save the model if it is the best so far\n",
    "    #        torch.save(model, f\"models/{model_name}\")\n",
    "    #        nosave = 0\n",
    "    #    else:\n",
    "    #            # Increment nosave\n",
    "    #        nosave += 1\n",
    "        # Otherwise, create it and save the model.\n",
    "    #except NameError:\n",
    "    #    best_val_loss = val_loss.item()\n",
    "    #    torch.save(model, f\"models/{model_name}\")\n",
    "    #    # stop the training if patience is used up\n",
    "    #if nosave > 5:\n",
    "    #    print(\"Patience is up!\")\n",
    "    #    break\n",
    "        # some print to see that it is running\n",
    "        #if (epoch + 1) % 10 == 0: # Comment or uncomment depending on how many epochs you want printed.\n",
    "    print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "    # \"best validation loss = {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"[INFO:] Finished traning!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
