{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If youâ€™re happy with the dataset, then load it\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from itertools import islice\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "Cross validation\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349\n",
    "text summarization\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961\n",
    "More text summarization\n",
    "\n",
    "\n",
    "PREPROCESSING DECISIONS:\n",
    "\n",
    "1. Labels at every tenth percentile.\n",
    "\n",
    "2. Max input sequence length - both standard BERT and max-doc-length as separate models.\n",
    "\n",
    "3. New-lines removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd37e6b95464fbf9269ec5dbf5bd4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b2e4417512470a909dd9f6eab3231d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85abc7cd8de34ce991e73b2f92b1eb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reddit_tifu/short to /home/coder/.cache/huggingface/datasets/reddit_tifu/short/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cc51cf46d8435fad72f070f1e05e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d5a02092ed4312a7a83298e88e4cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/79740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reddit_tifu downloaded and prepared to /home/coder/.cache/huggingface/datasets/reddit_tifu/short/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset reddit_tifu/long to /home/coder/.cache/huggingface/datasets/reddit_tifu/long/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d663dbb1cf59400980f153c5b61d8d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reddit_tifu downloaded and prepared to /home/coder/.cache/huggingface/datasets/reddit_tifu/long/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "tifu_short_raw = load_dataset(\"reddit_tifu\", \"short\", split=\"train\")\n",
    "tifu_long_raw = load_dataset(\"reddit_tifu\", \"long\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab35212e16342a595d1d97dd3652c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79740 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b6b622f2f34cac8b95ec5df552bdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42139 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add decimal labels to upvote ratio\n",
    "def add_columns(example):\n",
    "    example[\"upvote_ratio\"] = round(example[\"upvote_ratio\"], 1)\n",
    "    example[\"labels\"] = int(example[\"upvote_ratio\"] * 10)\n",
    "    return example\n",
    "\n",
    "tifu_short = tifu_short_raw.map(add_columns)\n",
    "\n",
    "tifu_long = tifu_long_raw.map(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "\n",
    "# Split into train, validation and test.\n",
    "dict1 = tifu_short.train_test_split(test_size=0.1)\n",
    "\n",
    "train_and_val = dict1[\"train\"]\n",
    "test_short = dict1[\"test\"]\n",
    "\n",
    "dict2 = train_and_val.train_test_split(test_size=0.2)\n",
    "\n",
    "train_short = dict2[\"train\"]\n",
    "val_short = dict2[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"documents\"], padding = True, truncation= True, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "train_input = train_short.map(tokenization, batched = True)\n",
    "val_input = val_short.map(tokenization, batched = True)\n",
    "test_input = test_short.map(tokenization, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.save_to_disk(\"preprocessed/train_short\")\n",
    "val_input.save_to_disk(\"preprocessed/val_short\")\n",
    "test_input.save_to_disk(\"preprocessed/test_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a model!\n",
    "\n",
    "def prepare_batch(token_ids, labels):\n",
    "    token_id_tensor = torch.LongTensor(np.array(token_ids)) # Turn list of lists into np.array into torch.LongTensor\n",
    "    labels = torch.LongTensor(np.array(labels))\n",
    "    output = tuple([token_id_tensor, labels])\n",
    "    return output\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_cl_short = load_from_disk(\"../preprocessed/train_short\")\n",
    "val_cl_short = load_from_disk(\"../preprocessed/val_short\")\n",
    "test_cl_short = load_from_disk(\"../preprocessed/test_short\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_cl_short = train_cl_short.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "train_cl_short.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "small_train = train_cl_short.select(range(1000))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(small_train, batch_size=32)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=11)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "from train import train_model\n",
    "train_model(model, train_dataloader, val_cl_short, optimizer, epochs, model_name = \"test_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
