{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup script first! Migrate to the correct folder and type bash setup.sh in the terminal.\n",
    "\n",
    "# Import loads of stuff (I don't think I even use half of these.)\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from itertools import islice\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "Cross validation\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349\n",
    "text summarization\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961\n",
    "More text summarization\n",
    "\n",
    "\n",
    "PREPROCESSING DECISIONS:\n",
    "\n",
    "1. Labels at every tenth percentile.\n",
    "\n",
    "2. Max input sequence length is 512, as per standard BERT procedure.\n",
    "\n",
    "3. New-lines removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "tifu_short_raw = load_dataset(\"reddit_tifu\", \"short\", split=\"train\")\n",
    "tifu_long_raw = load_dataset(\"reddit_tifu\", \"long\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decimal labels to upvote ratio\n",
    "def add_columns(example):\n",
    "    example[\"upvote_ratio\"] = round(example[\"upvote_ratio\"], 1)\n",
    "    example[\"labels\"] = int(example[\"upvote_ratio\"] * 10)\n",
    "    return example\n",
    "\n",
    "tifu_short = tifu_short_raw.map(add_columns)\n",
    "\n",
    "tifu_long = tifu_long_raw.map(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into train, validation and test.\n",
    "dict1 = tifu_short.train_test_split(test_size=0.1)\n",
    "\n",
    "train_and_val = dict1[\"train\"]\n",
    "test_short = dict1[\"test\"]\n",
    "\n",
    "dict2 = train_and_val.train_test_split(test_size=0.2)\n",
    "\n",
    "train_short = dict2[\"train\"]\n",
    "val_short = dict2[\"test\"]\n",
    "\n",
    "# Some light preprocessing:\n",
    "\n",
    "def remove_new_lines(example):\n",
    "    text = example[\"documents\"]\n",
    "    example[\"documents\"] = text.replace('\\n\\n', '. ').replace('\\n', '. ') # Replace double and single new lines with dots.\n",
    "    return example\n",
    "\n",
    "train_short = train_short.map(remove_new_lines)\n",
    "val_short = val_short.map(remove_new_lines)\n",
    "test_short = test_short.map(remove_new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization time!\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "## LOOK HERE! If you want to train on full text, run as it is. If you want to train on titles, change 'documents' to 'title'.\n",
    "def tokenization(example): \n",
    "    return tokenizer(example[\"documents\"], padding = True, truncation= True, return_tensors = \"pt\")\n",
    "\n",
    "# Tokenize and save to preprocessed. IMPORTANT: Don't type \"git add .\" ever! These files are too large to add to git, and\n",
    "# it becomes very problematic to change once you've added and committed them!!!!\n",
    "train_input = train_short.map(tokenization, batched = True)\n",
    "val_input = val_short.map(tokenization, batched = True)\n",
    "test_input = test_short.map(tokenization, batched = True)\n",
    "\n",
    "train_input.save_to_disk(\"preprocessed/train_short\")\n",
    "val_input.save_to_disk(\"preprocessed/val_short\")\n",
    "test_input.save_to_disk(\"preprocessed/test_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training script (and allow for changes to it)\n",
    "import importlib\n",
    "import train\n",
    "importlib.reload(train)\n",
    "\n",
    "# Let's train a model!\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Your preprocessed train, val and test sets.\n",
    "train_cl_short = load_from_disk(\"preprocessed/train_short\")\n",
    "val_cl_short = load_from_disk(\"preprocessed/val_short\")\n",
    "test_cl_short = load_from_disk(\"preprocessed/test_short\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The way we feed data to the model is using the dataloader class. It expects a very, very specific sort of input that looks like this:\n",
    "train_cl_short = train_cl_short.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "train_cl_short.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "# And this:\n",
    "val_cl_short = val_cl_short.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "val_cl_short.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Your input ids are your tokens, labels are the upvote ratio.\n",
    "\n",
    "\n",
    "# This model takes a long-ass time to train even a single epoch on.\n",
    "# These variables are here to ensure that you can test it on small samples.\n",
    "small_train = train_cl_short.select(range(32))\n",
    "small_val = val_cl_short.select(range(10))\n",
    "\n",
    "\n",
    "# Note: If you want to run on the full data, change 'small_train' and 'small_val' to\n",
    "# 'train_cl_short' and 'val_cl_short' respectively.\n",
    "# Then go on holiday, run a marathon, write a book, get married, etc. while it runs.\n",
    "train_dataloader = torch.utils.data.DataLoader(small_train, batch_size=32)\n",
    "val_dataloader = torch.utils.data.DataLoader(small_val, batch_size = len(small_val))\n",
    "\n",
    "#Import model and optimizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=11) # We have 11 labels.\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model for the very first time (if you want to load an already trained model\n",
    "# to keep training it, use the code blpck below.)\n",
    "\n",
    "## IMPORTANT! If you want the model to run until validation loss is minimized, epochs should just be set to False.\n",
    "# In practice, however, these models take so long that I doubt we will ever get to the minimum,\n",
    "# so specifying a number for epochs is probably a good idea.\n",
    "epochs = 2\n",
    "model_name = \"test_\"  # Replace with whatever else you want. The train function ensures that it is a .pt model.\n",
    "\n",
    "\n",
    "train.train_model(model, train_dataloader, val_dataloader, optimizer, epochs, model_name = \"test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO:] Training classifier...\n",
      "This is working for the  1 th time!\n",
      "epoch: 2, loss = 2.2769\n",
      "This is working for the  1 th time!\n",
      "epoch: 3, loss = 2.2769\n",
      "Max epochs reached\n",
      "[INFO:] Finished traning!\n"
     ]
    }
   ],
   "source": [
    "## Train existing model\n",
    "old_model_name = \"models/\" ## Add whatever your model is called. Remember, it's a .pt file!\n",
    "old_model = torch.load(old_model_name)\n",
    "train.train_model(old_model, train_dataloader, val_dataloader, optimizer, epochs, model_name = \"test_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
