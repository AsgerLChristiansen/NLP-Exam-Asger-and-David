{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup script first! Migrate to the correct folder and type bash setup.sh in the terminal.\n",
    "\n",
    "# Import loads of stuff (I don't think I even use half of these.)\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from itertools import islice\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "Cross validation\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349\n",
    "text summarization\n",
    "\n",
    "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961\n",
    "More text summarization\n",
    "\n",
    "\n",
    "PREPROCESSING DECISIONS:\n",
    "\n",
    "1. Labels at every tenth percentile.\n",
    "\n",
    "2. Max input sequence length is 512, as per standard BERT procedure.\n",
    "\n",
    "3. New-lines removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db577edf23e485296afc3a7a71feb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefc69cf9a70406e856079a350e76ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305be4883d8242f8af680d8e7e09e9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reddit_tifu/short to /home/coder/.cache/huggingface/datasets/reddit_tifu/short/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aae6586c55a46a485fc57cb1921169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59f513f516f49c2bb7322863b732620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/79740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reddit_tifu downloaded and prepared to /home/coder/.cache/huggingface/datasets/reddit_tifu/short/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset reddit_tifu/long to /home/coder/.cache/huggingface/datasets/reddit_tifu/long/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3fd83269ed42f7b4ffa83a2c61b867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reddit_tifu downloaded and prepared to /home/coder/.cache/huggingface/datasets/reddit_tifu/long/1.1.0/1c73fb08807b54ec26b025829b2a3d90c6f7466dac20801c825571af9514c049. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "tifu_short_raw = load_dataset(\"reddit_tifu\", \"short\", split=\"train\")\n",
    "tifu_long_raw = load_dataset(\"reddit_tifu\", \"long\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34efad9bb97045238cef69f3ecc3ae1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79740 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f631a679ac52425db0fc249a15a979df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42139 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add decimal labels to upvote ratio\n",
    "def add_columns(example):\n",
    "    example[\"upvote_ratio\"] = round(example[\"upvote_ratio\"], 1)\n",
    "    example[\"labels\"] = int(example[\"upvote_ratio\"] * 10)\n",
    "    return example\n",
    "\n",
    "tifu_short = tifu_short_raw.map(add_columns)\n",
    "\n",
    "tifu_long = tifu_long_raw.map(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d90cc9a05b54e36a5593ada07ffbec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30340 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd97451e34b4d7b92daaf2527c9ef16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7585 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd6cfc480a6418eb37b0c857e15ffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4214 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Split into train, validation and test.\n",
    "dict1 = tifu_long.train_test_split(test_size=0.1)\n",
    "\n",
    "train_and_val = dict1[\"train\"]\n",
    "test_long = dict1[\"test\"]\n",
    "\n",
    "dict2 = train_and_val.train_test_split(test_size=0.2)\n",
    "\n",
    "train_long = dict2[\"train\"]\n",
    "val_long = dict2[\"test\"]\n",
    "\n",
    "# Some light preprocessing:\n",
    "\n",
    "def remove_new_lines(example):\n",
    "    text = example[\"documents\"]\n",
    "    example[\"documents\"] = text.replace('\\n\\n', '. ').replace('\\n', '. ') # Replace double and single new lines with dots.\n",
    "    return example\n",
    "\n",
    "train_long = train_long.map(remove_new_lines)\n",
    "val_long = val_long.map(remove_new_lines)\n",
    "test_long = test_long.map(remove_new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5ebf68344b4b4384c679590478b24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9331fbfd4f4dd697a2df07fd45b8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd600a567da47f6b0670148d2131d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92374bcebf34f549905633c185edd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93882918d8814e4693de8f51c364b728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a96ed0f95945d784dfdf32b5325bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc502cca90c4435d85f34497020aca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fdc57d7d554d10a11eb1a03b738f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6727fea499409eac8ad928c379109a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd09d26d9a2475fbd1efb84e7b91c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization time!\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "## LOOK HERE! If you want to train on full text, run as it is. If you want to train on titles, change 'documents' to 'title'.\n",
    "def tokenization(example): \n",
    "    return tokenizer(example[\"tldr\"], padding  = 'max_length', truncation= True, return_tensors = \"pt\")\n",
    "\n",
    "# Tokenize and save to preprocessed. IMPORTANT: Don't type \"git add .\" ever! These files are too large to add to git, and\n",
    "# it becomes very problematic to change once you've added and committed them!!!!\n",
    "train_input = train_long.map(tokenization, batched = True)\n",
    "val_input = val_long.map(tokenization, batched = True)\n",
    "test_input = test_long.map(tokenization, batched = True)\n",
    "\n",
    "train_input.save_to_disk(\"preprocessed/train_long\")\n",
    "val_input.save_to_disk(\"preprocessed/val_long\")\n",
    "test_input.save_to_disk(\"preprocessed/test_long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /work/NLP-classroom/NLP-Exam-Asger-and-David/src/preprocessed/gen_tldr_1/cache-1c76d21392464ab0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1477e43df04338a1dde0a0a2fb4050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf8d843f98747cd858a4ffe9e43ad82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bddf7b8a52044d88eadb70e5c7401ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bb39ec7ec84337951ac56f496382d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb84210ac27547ffb7abe5d992d2a189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82745faba984a33a5e538073ab0de7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization time!\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# Your preprocessed train, val and test sets.\n",
    "one = load_from_disk(\"preprocessed/gen_tldr_1\")\n",
    "two = load_from_disk(\"preprocessed/gen_tldr_2\")\n",
    "three = load_from_disk(\"preprocessed/gen_tldr_3\")\n",
    "\n",
    "gen_tldr = concatenate_datasets([one,two,three])\n",
    "\n",
    "## LOOK HERE! If you want to train on full text, run as it is. If you want to train on titles, change 'documents' to 'title'.\n",
    "def tokenization(example): \n",
    "    return tokenizer(example[\"tldr\"], padding  = 'max_length', truncation= True, return_tensors = \"pt\")\n",
    "\n",
    "# Tokenize and save to preprocessed. IMPORTANT: Don't type \"git add .\" ever! These files are too large to add to git, and\n",
    "# it becomes very problematic to change once you've added and committed them!!!!\n",
    "gen_tldr_cleaned = gen_tldr.map(tokenization, batched = True)\n",
    "\n",
    "\n",
    "# Split into train, validation and test.\n",
    "dict1 = gen_tldr_cleaned.train_test_split(test_size=0.1)\n",
    "\n",
    "train_and_val = dict1[\"train\"]\n",
    "test = dict1[\"test\"]\n",
    "\n",
    "dict2 = train_and_val.train_test_split(test_size=0.2)\n",
    "\n",
    "train = dict2[\"train\"]\n",
    "val = dict2[\"test\"]\n",
    "\n",
    "train.save_to_disk(\"preprocessed/gen_tldr_train\")\n",
    "val.save_to_disk(\"preprocessed/gen_tldr_val\")\n",
    "test.save_to_disk(\"preprocessed/gen_tldr_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27072"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad96e9ec8d4d40e6ba3700cb6e45617b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import training script (and allow for changes to it)\n",
    "import importlib\n",
    "import train\n",
    "importlib.reload(train)\n",
    "\n",
    "# Let's train a model!\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Your preprocessed train, val and test sets.\n",
    "train_cl_long = load_from_disk(\"preprocessed/train_long\")\n",
    "val_cl_long = load_from_disk(\"preprocessed/val_long\")\n",
    "test_cl_long = load_from_disk(\"preprocessed/test_long\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The way we feed data to the model is using the dataloader class. It expects a very, very specific sort of input that looks like this:\n",
    "train_cl_long = train_cl_long.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "train_cl_long.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "# And this:\n",
    "val_cl_long = val_cl_long.remove_columns(['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'])\n",
    "val_cl_long.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Your input ids are your tokens, labels are the upvote ratio.\n",
    "\n",
    "\n",
    "# This model takes a long-ass time to train even a single epoch on.\n",
    "# These variables are here to ensure that you can test it on small samples.\n",
    "small_train = train_cl_long.select(range(32))\n",
    "small_val = val_cl_long.select(range(10))\n",
    "#train_dataloader = torch.utils.data.DataLoader(small_train, batch_size=32)\n",
    "#val_dataloader = torch.utils.data.DataLoader(small_val, batch_size = len(small_val))\n",
    "\n",
    "# Then go on holiday, run a marathon, write a book, get married, etc. while it runs.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_cl_long, batch_size=32)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_cl_long, batch_size = len(val_cl_long))\n",
    "\n",
    "#Import model and optimizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=11) # We have 11 labels.\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
